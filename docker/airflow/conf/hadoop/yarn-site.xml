<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- http://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-common/ClusterSetup.html -->
<configuration>

    <!-- Single hostname that can be set in place of setting all
    yarn.resourcemanager*address resources. Results in default ports
    for ResourceManager components: scheduler - 8030, resource-tracker - 8031,
    resourcemanager - 8032, admin - 8033, webapp - 8088. -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>

    <!-- Shuffle service that needs to be set for Map Reduce applications. -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- Memory and CPU constraints -->

    <!-- Amount of physical memory, in MB, that can be allocated for containers.
     It means the amount of memory YARN can utilize on this node and therefore
     this property should be lower than the total memory of that machine. -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>8192</value>
    </property>

    <!-- Resource manager can only allocate memory to containers in increments
    of "yarn.scheduler.minimum-allocation-mb" (1024 by default) and not exceed
    "yarn.scheduler.maximum-allocation-mb" (8192 by default).
    It should not be more then total memory of the Node.
    YARN processes each map or reduce task in a container so this
    param divided by param above is effectively how many jobs you can
    run concurrently. -->
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>

    <!-- And it can only allocate CPU vcores to containers
    in increments of "yarn.scheduler.minimum-allocation-vcores"
     and not exceed "yarn.scheduler.maximum-allocation-vcores". -->
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>4</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
    </property>

    <!-- Required for Hive on Spark
    https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started -->
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <!-- Note: do not reformat, any new lines between <value></value>
        tags will cause NoClassDefFound for FairScheduler-->
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>

    <!-- Getting "application is running beyond the 'VIRTUAL' memory limit"
    in Spark YARN otherwise-->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

</configuration>
